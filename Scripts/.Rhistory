"Tolima",
"Valle del Cauca")
) |>
tidyr::drop_na()
# departamento_1_sf pasa de "SHAPE" a "geometry"
names(departamentos_sf)[names(departamentos_sf) == "SHAPE"] <- "geometry"
departamentos_sf <- sf::st_as_sf(as.data.frame(departamentos_sf), sf_column_name = "geometry")
# Aseguramos que ambos datasets tengan la misma proyección
departamentos_sf <- st_transform(departamentos_sf, st_crs(ucs_rao_sf))
# Se unen los polígonos en uno solo
limite_poly <- st_union(departamentos_sf)
# Aplica un buffer pequeño (ejemplo: 1 km) para limitar efectos de borde
limite_buffer <- st_buffer(limite_poly, dist = 1000)
knitr::purl('05_analisis_glm_rao_colombia.qmd')
buffer_10km_sf <- st_buffer(limite_poly, dist = 10000) # buffer de 10 km
buffer_10km_gee <- sf_as_ee(buffer_100km_sf)
buffer_10km_gee <- sf_as_ee(buffer_10km_sf)
# Carga de librerías "inocentes"
pacman::p_load(char = c("here", "remotes", "sf", "geojsonio","dplyr", "ggplot2", "patchwork", "wesanderson", "qs"))
buffer_10km_sf <- st_buffer(limite_poly, dist = 10000) # buffer de 10 km
buffer_10km_gee <- sf_as_ee(buffer_10km_sf)
buffer_10km_bbox <- buffer_100km_gee$geometry()$bounds() #exporta rectángulo bbox
buffer_10km_sf <- st_buffer(limite_poly, dist = 10000) # buffer de 10 km
buffer_10km_gee <- sf_as_ee(buffer_10km_sf)
buffer_10km_bbox <- buffer_10km_gee$geometry()$bounds() #exporta rectángulo bbox
buffer_10km_bbox <- buffer_10km_gee$geometries()$bounds() #exporta rectángulo bbox
buffer_10km_bbox <- buffer_10km_gee$geometry()$bounds() #exporta rectángulo bbox
buffer_10km_bbox <- buffer_10km_gee$bounds() #exporta rectángulo bbox
target_crs <- "EPSG:9377"    # MAGNA-SIRGAS / Colombia Origen Nacional
target_scale <- 50           # Resolución de pixel objetivo (50 m)
export_folder <- "GEE_exports" # Carpeta destino en Google Drive
# 1. Selecciona DEM SRTM 30m y recorta al buffer del área de estudio
dem <- ee$Image("USGS/SRTMGL1_003")$clip(buffer_10km_bbox)
# 2. Visualiza en el visor antes de exportar (verifica recorte)
Map$setCenter(lon = -74, lat = 4, zoom = 6)
Map$addLayer(dem, visParams = list(min = 0, max = 3000), name = "DEM SRTM (nativo 30m)")
# 2. Visualiza en el visor antes de exportar (verifica recorte)
Map$setCenter(lon = -74, lat = 4, zoom = 5)
Map$addLayer(dem, visParams = list(min = 0, max = 3000), name = "DEM SRTM (nativo 30m)")
# 3. Remuestrea y reproyecta a 50m, CRS 9377 para garantizar co-registro con otras variables
dem_proj <- dem$reproject(crs = target_crs, scale = target_scale)
# 4. Exporta a Google Drive, con todos los parámetros del stack final
task_dem <- ee$batch$Export$image$toDrive(
image = dem_proj,                          # imagen a exportar
description = "DEM_SRTM_50m_9377",         # nombre descriptivo de la tarea
folder = export_folder,                    # carpeta en tu Google Drive
fileNamePrefix = "DEM_SRTM_50m_9377",      # nombre del archivo
region = buffer_10km_bbox,                # región a exportar (el buffer de 100 km)
scale = target_scale,                      # resolución (50 m)
crs = target_crs,                          # proyección
maxPixels = 1e13                           # límite para exportar imágenes grandes
)
# 5. Inicia la tarea de exportación (esto se procesa en la nube de GEE).
task_dem$start()
#target_crs <- "EPSG:9377"    # MAGNA-SIRGAS / Colombia Origen Nacional
target_crs <- "+proj=tmerc +lat_0=4 +lon_0=-73 +k=0.9992 +x_0=5000000 +y_0=2000000 +ellps=GRS80 +units=m +no_defs"
# 3. Remuestrea y reproyecta a 50m, CRS 9377 para garantizar co-registro con otras variables
dem_proj <- dem$reproject(crs = target_crs, scale = target_scale)
# 4. Exporta a Google Drive, con todos los parámetros del stack final
task_dem <- ee$batch$Export$image$toDrive(
image = dem_proj,                          # imagen a exportar
description = "DEM_SRTM_50m_9377",         # nombre descriptivo de la tarea
folder = export_folder,                    # carpeta en tu Google Drive
fileNamePrefix = "DEM_SRTM_50m_9377",      # nombre del archivo
region = buffer_10km_bbox,                # región a exportar (el buffer de 100 km)
scale = target_scale,                      # resolución (50 m)
crs = target_crs,                          # proyección
maxPixels = 1e13                           # límite para exportar imágenes grandes
)
# 5. Inicia la tarea de exportación (esto se procesa en la nube de GEE).
task_dem$start()
knitr::purl('05_analisis_glm_rao_colombia.qmd')
# 1. Selecciona DEM SRTM 30m y recorta al buffer del área de estudio
dem <- ee$Image("USGS/SRTMGL1_003")$clip(buffer_10km_bbox)
# 2. Visualiza en el visor antes de exportar (verifica recorte)
Map$setCenter(lon = -74, lat = 4, zoom = 5)
Map$addLayer(dem, visParams = list(min = 0, max = 3000), name = "DEM SRTM (nativo 30m)")
# 3. Remuestrea y reproyecta a 50m, CRS 9377 para garantizar co-registro con otras variables
dem_proj <- dem$reproject(crs = target_crs, scale = target_scale)
# 4. Exporta a Google Drive, con todos los parámetros del stack final
task_dem <- ee$batch$Export$image$toDrive(
image = dem_proj,                          # imagen a exportar
description = "DEM_SRTM_50m_4326",         # nombre descriptivo de la tarea
folder = export_folder,                    # carpeta en tu Google Drive
fileNamePrefix = "DEM_SRTM_50m_4326",      # nombre del archivo
region = buffer_10km_bbox,                # región a exportar (el buffer de 100 km)
scale = target_scale,                      # resolución (50 m)
crs = target_crs,                          # proyección
maxPixels = 1e13                           # límite para exportar imágenes grandes
)
# 5. Inicia la tarea de exportación (esto se procesa en la nube de GEE).
task_dem$start()
# Crea el buffer en metros en 9377
buffer_10km_sf <- st_buffer(limite_poly, dist = 10000)
# TRANSFORMA EL BUFFER A 4326 *ANTES* DE PASARLO A GEE
buffer_10km_sf_4326 <- st_transform(buffer_10km_sf, 4326)
# Convierte buffer a GEE
buffer_10km_gee <- sf_as_ee(buffer_10km_sf_4326)
buffer_10km_bbox <- buffer_10km_gee$bounds()
target_crs <- "EPSG:4326"    # Sistema soportado por GEE para exportación universal
target_scale <- 50           # Resolución de pixel objetivo (50 m)
export_folder <- "GEE_exports" # Carpeta destino en Google Drive "My Drive"
knitr::purl('05_analisis_glm_rao_colombia.qmd')
# 3. Remuestrea y reproyecta a 50m, CRS 9377 para garantizar co-registro con otras variables
dem_proj <- dem$reproject(crs = target_crs, scale = target_scale)
# 4. Exporta a Google Drive, con todos los parámetros del stack final
task_dem <- ee$batch$Export$image$toDrive(
image = dem_proj,                          # imagen a exportar
description = "DEM_SRTM_50m_4326",         # nombre descriptivo de la tarea
folder = export_folder,                    # carpeta en tu Google Drive
fileNamePrefix = "DEM_SRTM_50m_4326",      # nombre del archivo
region = buffer_10km_bbox,                # región a exportar (el buffer de 100 km)
scale = target_scale,                      # resolución (50 m)
crs = target_crs,                          # proyección
maxPixels = 1e13                           # límite para exportar imágenes grandes
)
# 5. Inicia la tarea de exportación (esto se procesa en la nube de GEE).
task_dem$start()
# Calcula pendiente (grados) usando el DEM ya recortado
slope <- ee$Terrain$slope(dem)
# Visualiza para revisar el resultado antes de exportar
Map$addLayer(slope, visParams = list(min = 0, max = 45, palette = c("white", "yellow", "red")), name = "Pendiente")
# Calcula pendiente (grados) usando el DEM ya recortado
slope <- ee$Terrain$slope(dem)
# Visualiza para revisar el resultado antes de exportar
Map$addLayer(slope, visParams = list(min = 0, max = 45, palette = c("white", "yellow", "red")), name = "Pendiente")
# Reproyecta a 50 m y CRS 4326
slope_proj <- slope$reproject(crs = target_crs, scale = target_scale)
# Exporta a Google Drive
task_slope <- ee$batch$Export$image$toDrive(
image = slope_proj,
description = "Slope_50m_4326",
folder = export_folder,
fileNamePrefix = "Slope_50m_4326",
region = buffer_10km_bbox,
scale = target_scale,
crs = target_crs,
maxPixels = 1e13
)
task_slope$start()
# Carga y convierte la banda térmica a Kelvin, filtra fechas y recorta al área
landsat_lst <- ee$ImageCollection("LANDSAT/LC08/C02/T1_L2")$
filterBounds(buffer_10km_bbox)$
filterDate("2013-01-01", "2023-01-01")$
map(function(img) img$select("ST_B10")$multiply(0.00341802)$add(149.0))
lst_median <- landsat_lst$median()$clip(buffer_10km_bbox)
# Visualiza la mediana antes de exportar
Map$addLayer(lst_median, visParams = list(min = 270, max = 320, palette = c("blue", "white", "red")), name = "LST mediana")
# Función para enmascarar nubes/agua usando QA_PIXEL antes de procesar LST
maskL8LST <- function(img) {
# QA_PIXEL: bit 1 = Dilated Cloud, bit 3 = Cloud, bit 4 = Cloud Shadow, bit 5 = Snow, bit 7 = Water
qa <- img$select("QA_PIXEL")
# Bits a filtrar (Cloud, Dilated Cloud, Cloud Shadow, Water)
cloud_mask <- qa$bitwiseAnd(1 << 1)$eq(0)$
# 1. Selecciona la colección de imágenes de Landsat 8 Collection 2 Nivel 2 (procesamiento superficial)
landsat_col <- ee$ImageCollection("LANDSAT/LC08/C02/T1_L2")
# 2. Filtra la colección espacialmente al buffer definido
landsat_col <- landsat_col$filterBounds(buffer_10km_bbox)
# 3. Filtra la colección temporalmente al rango de fechas deseado (2013–2023)
landsat_col <- landsat_col$filterDate("2013-01-01", "2023-01-01")
# 4. Aplica una función a cada imagen para:
#    a) Seleccionar la banda térmica "ST_B10"
#    b) Convertirla a Kelvin según el factor de escala Landsat 8 (ver USGS)
landsat_kelvin <- landsat_col$map(function(img) {
img$select("ST_B10")$multiply(0.00341802)$add(149.0)$copyProperties(img, img$propertyNames())
})
# 5. Calcula la mediana temporal pixel a pixel (mediana multianual de temperatura superficial)
lst_median <- landsat_kelvin$median()$clip(buffer_10km_bbox)
# 6. Visualiza para verificar valores y cobertura antes de exportar
Map$addLayer(lst_median, visParams = list(min = 270, max = 320, palette = c("blue", "white", "red")), name = "LST (Landsat 8 mediana)")
# 7. Reproyecta a 50 m y CRS 4326 para garantizar co-registro
lst_proj <- lst_median$reproject(crs = target_crs, scale = target_scale)
# 8. Exporta a Google Drive
task_lst <- ee$batch$Export$image$toDrive(
image = lst_proj,
description = "LST_Landsat_median_50m_4326",
folder = export_folder,
fileNamePrefix = "LST_Landsat_median_50m_4326",
region = buffer_10km_bbox,
scale = target_scale,
crs = target_crs,
maxPixels = 1e13
)
task_lst$start()
# 1. Selecciona la colección Sentinel-1 GRD (Ground Range Detected)
s1_col <- ee$ImageCollection("COPERNICUS/S1_GRD")
# 2. Filtra espacialmente al buffer
s1_col <- s1_col$filterBounds(buffer_10km_bbox)
# 3. Filtra temporalmente al rango deseado (2015–2024)
s1_col <- s1_col$filterDate("2015-01-01", "2024-01-01")
# 4. Filtra imágenes que tengan ambas polarizaciones VV y VH
s1_col <- s1_col$
filter(ee$Filter$listContains("transmitterReceiverPolarisation", "VV"))$
filter(ee$Filter$listContains("transmitterReceiverPolarisation", "VH"))
# 5. Filtra solo imágenes en órbita ascendente (opcional, para reducir sesgo angular)
s1_col <- s1_col$filter(ee$Filter$eq("orbitProperties_pass", "ASCENDING"))
# 6. Selecciona solo las bandas VH y VV
s1_col <- s1_col$select(c("VH", "VV"))
# 7. Calcula la razón VH/VV para cada imagen de la colección
vh_vv_ratio <- s1_col$map(function(img) {
img$select("VH")$divide(img$select("VV"))$rename("VH_VV_ratio")$copyProperties(img, img$propertyNames())
})
# 8. Calcula la mediana temporal pixel a pixel
vh_vv_median <- vh_vv_ratio$median()$clip(buffer_10km_bbox)
# 9. Visualiza antes de exportar
Map$addLayer(vh_vv_median, visParams = list(min = 0, max = 1, palette = c("red", "white", "blue")), name = "Sentinel-1 VH/VV (mediana)")
# 10. Reproyecta y exporta
vh_vv_proj <- vh_vv_median$reproject(crs = target_crs, scale = target_scale)
task_vhvv <- ee$batch$Export$image$toDrive(
image = vh_vv_proj,
description = "Sentinel1_VH_VV_median_50m_4326",
folder = export_folder,
fileNamePrefix = "Sentinel1_VH_VV_median_50m_4326",
region = buffer_10km_bbox,
scale = target_scale,
crs = target_crs,
maxPixels = 1e13
)
task_vhvv$start()
# 9. Visualiza antes de exportar
Map$addLayer(vh_vv_median, visParams = list(min = 0, max = 1, palette = c("red", "white", "blue")), name = "Sentinel-1 VH/VV (mediana)")
# 1. Selecciona la colección CHIRPS diaria (UCSB-CHG/CHIRPS/DAILY)
chirps_col <- ee$ImageCollection("UCSB-CHG/CHIRPS/DAILY")
# 2. Filtra al área de trabajo (buffer)
chirps_col <- chirps_col$filterBounds(buffer_10km_bbox)
# 3. Filtra al rango de fechas de interés (2000–2020)
chirps_col <- chirps_col$filterDate("2000-01-01", "2020-12-31")
# 4. Calcula la mediana multianual pixel a pixel
precip_median <- chirps_col$median()$clip(buffer_10km_bbox)
# 5. Visualiza antes de exportar
Map$addLayer(precip_median, visParams = list(min = 0, max = 4000, palette = c("white", "blue")), name = "CHIRPS Precipitación mediana")
# 6. Reproyecta y exporta a 50m y CRS 4326
precip_proj <- precip_median$reproject(crs = target_crs, scale = target_scale)
task_precip <- ee$batch$Export$image$toDrive(
image = precip_proj,
description = "CHIRPS_Precip_median_50m_4326",
folder = export_folder,
fileNamePrefix = "CHIRPS_Precip_median_50m_4326",
region = buffer_10km_bbox,
scale = target_scale,
crs = target_crs,
maxPixels = 1e13
)
task_precip$start()
# Para renderizar el documento
knitr::opts_chunk$set(echo = TRUE)
# Para cargar librerias se verifica pacman
if ("pacman" %in% installed.packages() == FALSE) install.packages("pacman")
# Se cargan las librerias
pacman::p_load(char = c(
"here", #manejo de rutas
"sf", #manipulación de dats espaciales
"dplyr", #procesamiento de data frames
"terra", #procesamiento raster
"ggplot2",  #graficación
"patchwork", #mosaicos gráficos
"rasterdiv", #para calcular raster
"wesanderson", #paleta de colores
"qs" #escribir y leer rápidamente objetos R
)
)
#Paleta de colores
pal <- wes_palette("Zissou1", 100, type = "continuous")
# Corre script externo
source(here::here("Scripts", "00_funcion_carga_ucs_procesadas_qs.R"), encoding = "UTF-8")
#Se crea una variable Qdens que expresa la densidad de Q
ucs_rao_sf <- ucs_rao_sf |>
tidyr::drop_na(Q) |>
# se deja un mínimo diferente de cero para trabajar con logaritmos
dplyr::mutate(
Q = case_when(Q == 0 ~ 0.000001, TRUE ~ Q),
Qdens = Q/AREA_HA,
log_Qdens = log(Qdens)
)
head(ucs_rao_sf)
# Crea un objeto tipo función al ejecutar un  script externo
source("00_funcion_carga_soilgrids.R")
# Se llama la función con los argumentos adaptados al proyecto
stack_suelo <- descargar_soilgrids_stack(
vars = c("bdod", "sand", "silt", "clay", "soc", "cec", "phh2o"),
depths = c("0-5cm", "5-15cm", "15-30cm", "60-100cm"),
stats = c("mean"),
resolucion = c(250, 250),
#define ruta de descarga y verifica si ya existen los archivos
ruta_vrt = here::here("Data", "OUT_SoilGrids_vrt")
)
# Definir la ruta de salida
out_raster <- here("Data", "OUT_soilgrids_stack.tif")
# Verificar si el archivo ya existe antes de escribir
if (!file.exists(out_raster)) {
writeRaster(stack_suelo, filename = out_raster, overwrite = TRUE)
} else {
message("El archivo ya existe, no se sobrescribirá.")
}
# Cargar el archivo .tif como un SpatRaster
stack_suelo_tif <- rast(out_raster)
#Se normalizan las bandas
normalize_band <- function(x) (x - mean(x, na.rm=TRUE)) / sd(x, na.rm=TRUE)
stack_suelo_z <- app(stack_suelo_tif, normalize_band)
#Se compara con
plot(stack_suelo_z[[c(1, 8, 15, 22)]])
# Convierte el stack a una lista de SpatRaster (cada elemento una banda)
soilgrids_z_list <- lapply(1:nlyr(stack_suelo_z), function(i) stack_suelo_z[[i]])
# Calculo de rao multidimensional
rao_51 <- paRao(
x = soilgrids_z_list,   # Usa la lista estandarizada
window = 5, #aprox 12.7 km de lado. La ventana debe ser de lado impar
alpha = 1, #peso de la matriz de distancia con media artimetica
na.tolerance = 1,
method = "multidimension",
rasterOut = TRUE,
simplify = 2 #número de cifras decimales
)
knitr::purl('05_analisis_glm_rao_colombia.qmd')
?ee_as_raster
library(rgee)
# Corre script externo para cargar
source(here::here("Scripts", "00_funcion_carga_ucs_procesadas_qs.R"), encoding = "UTF-8")
# Asignación de id único para cada polígono
ucs_rao_sf <- ucs_rao_sf 1> mutate(id = row_number())
#Para exportar como .R plano
# knitr::purl('05_analisis_glm_rao_colombia.qmd')
# Para cargar librerias se verifica pacman
if ("pacman" %in% installed.packages() == FALSE) install.packages("pacman")
# Carga de librerías "inocentes"
pacman::p_load(char = c("here", "remotes", "sf", "geojsonio","dplyr", "ggplot2", "patchwork", "wesanderson", "qs"))
# ==== Creación de ambiente Miniconda para reticulate ============================
#Esto solo se necesita hacerlo una vez.
#reticulate::conda_create("rgee_py", packages = "python=3.12")
reticulate::py_config()  # Verifica que el python sea Miniconda3/envs/rgee_py/python.exe
ee_check() # Check non-R dependencies
# ==== Selección entorno ya existente antes de cualquier llamado que use Python ====
reticulate::use_condaenv("rgee_py", required = TRUE)
#Para exportar como .R plano
# knitr::purl('05_analisis_glm_rao_colombia.qmd')
# Para cargar librerias se verifica pacman
if ("pacman" %in% installed.packages() == FALSE) install.packages("pacman")
# Carga de librerías "inocentes"
pacman::p_load(char = c("here", "remotes", "sf", "geojsonio","dplyr", "ggplot2", "patchwork", "wesanderson", "qs"))
# ==== Creación de ambiente Miniconda para reticulate ============================
#Esto solo se necesita hacerlo una vez.
#reticulate::conda_create("rgee_py", packages = "python=3.12")
reticulate::py_config()  # Verifica que el python sea Miniconda3/envs/rgee_py/python.exe
# ==== Selección entorno ya existente antes de cualquier llamado que use Python ====
reticulate::use_condaenv("rgee_py", required = TRUE)
source("~/2025_UNAL_PEDODIV/pedodiv-colombia/Scripts/00_miniconda_rgee_setup.R")
# Para cargar librerias se verifica pacman
if ("pacman" %in% installed.packages() == FALSE) install.packages("pacman")
# Carga de librerías "inocentes"
pacman::p_load(char = c("here", "remotes", "sf", "geojsonio","dplyr", "ggplot2", "patchwork", "wesanderson", "qs"))
# Carga librerías que usan Python
library(reticulate)
library(rgee)
library(googledrive)
# ==== Autenticación y backend Python ====
ee_clean_user_credentials()      # Limpia credenciales de GEE
ee_clean_pyenv()           # Limpia variables de entorno de reticulate
reticulate::py_run_string("import ee; ee.Authenticate()")
reticulate::py_run_string("import ee; ee.Initialize(project='even-electron-461718-g2')")
# ==== Inicializa rgee (ignora asset_home si aparece, ya existe) ====
tryCatch(
rgee::ee_Initialize(drive = FALSE, project = 'even-electron-461718-g2'),
error = function(e) message("Si pide asset home y ya existe, solo ESC y sigue")
)
#Para exportar como .R plano
# knitr::purl('05_analisis_glm_rao_colombia.qmd')
# Para cargar librerias se verifica pacman
if ("pacman" %in% installed.packages() == FALSE) install.packages("pacman")
# Carga de librerías "inocentes"
pacman::p_load(char = c("here", "remotes", "sf", "geojsonio","dplyr", "ggplot2", "patchwork", "wesanderson", "qs"))
#Selección entorno ya existente antes de cualquier llamado que use Python: only needed while the interpreter path was not yet fixed
#reticulate::use_condaenv("rgee_py", required = TRUE)
#reticulate::py_config() # # Verifica que el python sea Miniconda3/envs/rgee_py/python.exe
# Carga librerías que usan Python
library(reticulate)
library(rgee)
library(googledrive)
# ==== Autenticación y backend Python ====
ee_clean_user_credentials()      # Limpia credenciales de GEE
#Para exportar como .R plano
# knitr::purl('05_analisis_glm_rao_colombia.qmd')
# Para cargar librerias se verifica pacman
if ("pacman" %in% installed.packages() == FALSE) install.packages("pacman")
# Carga de librerías "inocentes"
pacman::p_load(char = c("here", "remotes", "sf", "geojsonio","dplyr", "ggplot2", "patchwork", "wesanderson", "qs"))
#Selección entorno ya existente antes de cualquier llamado que use Python: only needed while the interpreter path was not yet fixed
reticulate::use_condaenv("rgee_py", required = TRUE)
#reticulate::py_config() # # Verifica que el python sea Miniconda3/envs/rgee_py/python.exe
# Carga librerías que usan Python
library(reticulate)
library(rgee)
library(googledrive)
# ==== Autenticación y backend Python ====
ee_clean_user_credentials()      # Limpia credenciales de GEE
ee_clean_pyenv()           # Limpia variables de entorno de reticulate
reticulate::py_run_string("import ee; ee.Authenticate()")
reticulate::py_run_string("import ee; ee.Initialize(project='even-electron-461718-g2')")
# ==== Inicializa rgee (ignora asset_home si aparece, ya existe) ====
tryCatch(
rdee::ee_Initialize(
drive       = FALSE,                         # or TRUE if you want Drive export
project     = "even-electron-461718-g2",     # your Cloud Project
asset_home  = "users/cmguiob_g",             # <-- tell rgee the folder exists
quiet       = TRUE
),
error = function(e) message("Si pide asset home y ya existe, solo ESC y sigue")
)
# Se consultan datos de DEM
img <- ee$Image("USGS/SRTMGL1_003")
#Consulta que propiedades están disponibles
img$propertyNames()$getInfo()
# Corre script externo para cargar
source(here::here("Scripts", "00_funcion_carga_ucs_procesadas_qs.R"), encoding = "UTF-8")
# Asignación de id único para cada polígono
ucs_rao_sf <- ucs_rao_sf |> mutate(id = row_number())
head(ucs_rao_sf)
# Define ruta y nombre de capa de geopackage de departamentos
deptos_ruta <- here("Data", "INP_departamentos_IGAC_Abril_2025.gpkg")
capa_nombre_deptos <- sf::st_layers(deptos_ruta)$name[1]
# Carga geopackage de dpartamentos
departamentos_sf <- sf::st_read(
deptos_ruta,
layer = capa_nombre_deptos,
quiet = TRUE
) |>
# Se seleccionan 21 departamentos de la zona Andina, Caribe y Pacífica
dplyr::filter(DeNombre %in% c(
"Antioquia",
"Atlántico",
"Bolívar",
"Boyacá",
"Caldas",
"Cauca",
"Cesar",
"Chocó",
"Córdoba",
"Cundinamarca",
"Huila",
"La Guajira",
"Magdalena",
"Nariño",
"Norte de Santander",
"Quindío",
"Risaralda",
"Santander",
"Sucre",
"Tolima",
"Valle del Cauca")
) |>
tidyr::drop_na()
# departamento_1_sf pasa de "SHAPE" a "geometry"
names(departamentos_sf)[names(departamentos_sf) == "SHAPE"] <- "geometry"
departamentos_sf <- sf::st_as_sf(as.data.frame(departamentos_sf), sf_column_name = "geometry")
# Aseguramos que ambos datasets tengan la misma proyección
departamentos_sf <- st_transform(departamentos_sf, st_crs(ucs_rao_sf))
# Se unen los polígonos en uno solo
limite_poly <- st_union(departamentos_sf)
ucs_ee     <- sf_as_ee(ucs_rao_sf)
estudio_ee <- sf_as_ee(limite_poly)
# Carga y suavizado del DEM SRTM 30 m
dem_orig   <- ee$Image("USGS/SRTMGL1_003")$rename("DEM")$clip(estudio_ee)
gauss      <- ee$Kernel$gaussian(radius=3, sigma=2, units="pixels", normalize=TRUE)
dem_smooth <- dem_orig$convolve(gauss)$resample("bilinear")
# 1. Centrar el visor en tu área de interés
Map$setCenter(lon = -74, lat = 4, zoom = 6)
# 2. Añadir el DEM original (sin suavizar)
Map$addLayer(
dem_orig,
visParams = list(min = 0, max = 3000,
palette = c("blue","green","yellow","red")),
name = "DEM original"
)
# 3. Añadir el DEM suavizado
Map$addLayer(
dem_smooth,
visParams = list(min = 0, max = 3000,
palette = c("blue","green","yellow","red")),
name = "DEM suavizado"
)
# 7. Importar y ejecutar TAGEE en GEE para atributos de terreno
tagee      <- ee$call("require", "users/zecojls/TAGEE:TAGEE-functions")
reticulate::py_last_error()
#Para exportar como .R plano
# knitr::purl('05_analisis_glm_hotspots.qmd')
if (!"pacman" %in% installed.packages()) install.packages("pacman")
pacman::p_load(here, remotes, sf, geojsonio, dplyr, ggplot2,
patchwork, wesanderson, qs)
# Selección entorno ya existente antes de cualquier llamado que use Python
reticulate::use_condaenv("rgee_py", required = TRUE)
## Librerías que usan Python
library(reticulate)
library(rgee)
library(googledrive)
# ==== Autenticación y backend Python ====
ee_clean_user_credentials()      # Limpia credenciales de GEE
ee_clean_pyenv()           # Limpia variables de entorno de reticulate
reticulate::py_run_string("import ee; ee.Authenticate()")
reticulate::py_run_string("import ee; ee.Initialize(project='even-electron-461718-g2')")
# Se consultan datos de DEM
img <- ee$Image("USGS/SRTMGL1_003")
#Consulta que propiedades están disponibles
img$propertyNames()$getInfo()
# Consultar una propiedad específica, e.g. keywords
img$get("keywords")$getInfo()
# Corre script externo para cargar
source(here::here("Scripts", "00_funcion_carga_ucs_procesadas_qs.R"), encoding = "UTF-8")
# Asignación de id único para cada polígono
ucs_rao_sf <- ucs_rao_sf |>
sf::st_make_valid() |> #valida geometrias problemáticas
dplyr::select(id_creado, UCSuelo, AREA_HA)
ucs_rao_sf <- ucs_rao_sf[!st_is_empty(ucs_rao_sf), ]
plot(st_geometry(ucs_rao_sf))
