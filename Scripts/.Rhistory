# Para cargar librerias se verifica pacman
if ("pacman" %in% installed.packages() == FALSE) install.packages("pacman")
# Se cargan las librerias
pacman::p_load(char = c(
"here", #manejo de rutas
"sf", #manipulación de dats espaciales
"dplyr", #procesamiento de data frames
"terra", #procesamiento raster
"ggplot2",  #graficación
"patchwork", #mosaicos gráficos
"rasterdiv", #para calcular raster
"wesanderson", #paleta de colores
"qs" #escribir y leer rápidamente objetos R
)
)
#Paleta de colores
pal <- wes_palette("Zissou1", 100, type = "continuous")
# Corre script externo
source(here::here("Scripts", "00_funcion_carga_ucs_procesadas_qs.R"), encoding = "UTF-8")
#Se crea una variable Qdens que expresa la densidad de Q
ucs_rao_sf <- ucs_rao_sf |>
tidyr::drop_na(Q) |>
# se deja un mínimo diferente de cero para trabajar con logaritmos
dplyr::mutate(
Q = case_when(Q == 0 ~ 0.000001, TRUE ~ Q),
Qdens = Q/AREA_HA,
log_Qdens = log(Qdens)
)
head(ucs_rao_sf)
# Crea un objeto tipo función al ejecutar un  script externo
source("00_funcion_carga_soilgrids.R")
# Se llama la función con los argumentos adaptados al proyecto
stack_suelo <- descargar_soilgrids_stack(
vars = c("bdod", "sand", "silt", "clay", "soc", "cec", "phh2o"),
depths = c("0-5cm", "5-15cm", "15-30cm", "60-100cm"),
stats = c("mean"),
resolucion = c(250, 250),
#define ruta de descarga y verifica si ya existen los archivos
ruta_vrt = here::here("Data", "OUT_SoilGrids_vrt")
)
# Definir la ruta de salida
out_raster <- here("Data", "OUT_soilgrids_stack.tif")
# Verificar si el archivo ya existe antes de escribir
if (!file.exists(out_raster)) {
writeRaster(stack_suelo, filename = out_raster, overwrite = TRUE)
} else {
message("El archivo ya existe, no se sobrescribirá.")
}
# Cargar el archivo .tif como un SpatRaster
stack_suelo_tif <- rast(out_raster)
#Se normalizan las bandas
normalize_band <- function(x) (x - mean(x, na.rm=TRUE)) / sd(x, na.rm=TRUE)
stack_suelo_z <- app(stack_suelo_tif, normalize_band)
#Se compara con
plot(stack_suelo_z[[c(1, 8, 15, 22)]])
# Convierte el stack a una lista de SpatRaster (cada elemento una banda)
soilgrids_z_list <- lapply(1:nlyr(stack_suelo_z), function(i) stack_suelo_z[[i]])
# Calculo de rao multidimensional
rao_51 <- paRao(
x = soilgrids_z_list,   # Usa la lista estandarizada
window = 5, #aprox 12.7 km de lado. La ventana debe ser de lado impar
alpha = 1, #peso de la matriz de distancia con media artimetica
na.tolerance = 1,
method = "multidimension",
rasterOut = TRUE,
simplify = 2 #número de cifras decimales
)
knitr::purl('05_analisis_glm_rao_colombia.qmd')
?ee_as_raster
library(rgee)
# Corre script externo para cargar
source(here::here("Scripts", "00_funcion_carga_ucs_procesadas_qs.R"), encoding = "UTF-8")
# Asignación de id único para cada polígono
ucs_rao_sf <- ucs_rao_sf 1> mutate(id = row_number())
#Para exportar como .R plano
# knitr::purl('05_analisis_glm_rao_colombia.qmd')
# Para cargar librerias se verifica pacman
if ("pacman" %in% installed.packages() == FALSE) install.packages("pacman")
# Carga de librerías "inocentes"
pacman::p_load(char = c("here", "remotes", "sf", "geojsonio","dplyr", "ggplot2", "patchwork", "wesanderson", "qs"))
# ==== Creación de ambiente Miniconda para reticulate ============================
#Esto solo se necesita hacerlo una vez.
#reticulate::conda_create("rgee_py", packages = "python=3.12")
reticulate::py_config()  # Verifica que el python sea Miniconda3/envs/rgee_py/python.exe
ee_check() # Check non-R dependencies
# ==== Selección entorno ya existente antes de cualquier llamado que use Python ====
reticulate::use_condaenv("rgee_py", required = TRUE)
#Para exportar como .R plano
# knitr::purl('05_analisis_glm_rao_colombia.qmd')
# Para cargar librerias se verifica pacman
if ("pacman" %in% installed.packages() == FALSE) install.packages("pacman")
# Carga de librerías "inocentes"
pacman::p_load(char = c("here", "remotes", "sf", "geojsonio","dplyr", "ggplot2", "patchwork", "wesanderson", "qs"))
# ==== Creación de ambiente Miniconda para reticulate ============================
#Esto solo se necesita hacerlo una vez.
#reticulate::conda_create("rgee_py", packages = "python=3.12")
reticulate::py_config()  # Verifica que el python sea Miniconda3/envs/rgee_py/python.exe
# ==== Selección entorno ya existente antes de cualquier llamado que use Python ====
reticulate::use_condaenv("rgee_py", required = TRUE)
source("~/2025_UNAL_PEDODIV/pedodiv-colombia/Scripts/00_miniconda_rgee_setup.R")
# Para cargar librerias se verifica pacman
if ("pacman" %in% installed.packages() == FALSE) install.packages("pacman")
# Carga de librerías "inocentes"
pacman::p_load(char = c("here", "remotes", "sf", "geojsonio","dplyr", "ggplot2", "patchwork", "wesanderson", "qs"))
# Carga librerías que usan Python
library(reticulate)
library(rgee)
library(googledrive)
# ==== Autenticación y backend Python ====
ee_clean_user_credentials()      # Limpia credenciales de GEE
ee_clean_pyenv()           # Limpia variables de entorno de reticulate
reticulate::py_run_string("import ee; ee.Authenticate()")
reticulate::py_run_string("import ee; ee.Initialize(project='even-electron-461718-g2')")
# ==== Inicializa rgee (ignora asset_home si aparece, ya existe) ====
tryCatch(
rgee::ee_Initialize(drive = FALSE, project = 'even-electron-461718-g2'),
error = function(e) message("Si pide asset home y ya existe, solo ESC y sigue")
)
#Para exportar como .R plano
# knitr::purl('05_analisis_glm_rao_colombia.qmd')
# Para cargar librerias se verifica pacman
if ("pacman" %in% installed.packages() == FALSE) install.packages("pacman")
# Carga de librerías "inocentes"
pacman::p_load(char = c("here", "remotes", "sf", "geojsonio","dplyr", "ggplot2", "patchwork", "wesanderson", "qs"))
#Selección entorno ya existente antes de cualquier llamado que use Python: only needed while the interpreter path was not yet fixed
#reticulate::use_condaenv("rgee_py", required = TRUE)
#reticulate::py_config() # # Verifica que el python sea Miniconda3/envs/rgee_py/python.exe
# Carga librerías que usan Python
library(reticulate)
library(rgee)
library(googledrive)
# ==== Autenticación y backend Python ====
ee_clean_user_credentials()      # Limpia credenciales de GEE
#Para exportar como .R plano
# knitr::purl('05_analisis_glm_rao_colombia.qmd')
# Para cargar librerias se verifica pacman
if ("pacman" %in% installed.packages() == FALSE) install.packages("pacman")
# Carga de librerías "inocentes"
pacman::p_load(char = c("here", "remotes", "sf", "geojsonio","dplyr", "ggplot2", "patchwork", "wesanderson", "qs"))
#Selección entorno ya existente antes de cualquier llamado que use Python: only needed while the interpreter path was not yet fixed
reticulate::use_condaenv("rgee_py", required = TRUE)
#reticulate::py_config() # # Verifica que el python sea Miniconda3/envs/rgee_py/python.exe
# Carga librerías que usan Python
library(reticulate)
library(rgee)
library(googledrive)
# ==== Autenticación y backend Python ====
ee_clean_user_credentials()      # Limpia credenciales de GEE
ee_clean_pyenv()           # Limpia variables de entorno de reticulate
reticulate::py_run_string("import ee; ee.Authenticate()")
reticulate::py_run_string("import ee; ee.Initialize(project='even-electron-461718-g2')")
# ==== Inicializa rgee (ignora asset_home si aparece, ya existe) ====
tryCatch(
rdee::ee_Initialize(
drive       = FALSE,                         # or TRUE if you want Drive export
project     = "even-electron-461718-g2",     # your Cloud Project
asset_home  = "users/cmguiob_g",             # <-- tell rgee the folder exists
quiet       = TRUE
),
error = function(e) message("Si pide asset home y ya existe, solo ESC y sigue")
)
# Se consultan datos de DEM
img <- ee$Image("USGS/SRTMGL1_003")
#Consulta que propiedades están disponibles
img$propertyNames()$getInfo()
# Corre script externo para cargar
source(here::here("Scripts", "00_funcion_carga_ucs_procesadas_qs.R"), encoding = "UTF-8")
# Asignación de id único para cada polígono
ucs_rao_sf <- ucs_rao_sf |> mutate(id = row_number())
head(ucs_rao_sf)
# Define ruta y nombre de capa de geopackage de departamentos
deptos_ruta <- here("Data", "INP_departamentos_IGAC_Abril_2025.gpkg")
capa_nombre_deptos <- sf::st_layers(deptos_ruta)$name[1]
# Carga geopackage de dpartamentos
departamentos_sf <- sf::st_read(
deptos_ruta,
layer = capa_nombre_deptos,
quiet = TRUE
) |>
# Se seleccionan 21 departamentos de la zona Andina, Caribe y Pacífica
dplyr::filter(DeNombre %in% c(
"Antioquia",
"Atlántico",
"Bolívar",
"Boyacá",
"Caldas",
"Cauca",
"Cesar",
"Chocó",
"Córdoba",
"Cundinamarca",
"Huila",
"La Guajira",
"Magdalena",
"Nariño",
"Norte de Santander",
"Quindío",
"Risaralda",
"Santander",
"Sucre",
"Tolima",
"Valle del Cauca")
) |>
tidyr::drop_na()
# departamento_1_sf pasa de "SHAPE" a "geometry"
names(departamentos_sf)[names(departamentos_sf) == "SHAPE"] <- "geometry"
departamentos_sf <- sf::st_as_sf(as.data.frame(departamentos_sf), sf_column_name = "geometry")
# Aseguramos que ambos datasets tengan la misma proyección
departamentos_sf <- st_transform(departamentos_sf, st_crs(ucs_rao_sf))
# Se unen los polígonos en uno solo
limite_poly <- st_union(departamentos_sf)
ucs_ee     <- sf_as_ee(ucs_rao_sf)
estudio_ee <- sf_as_ee(limite_poly)
# Carga y suavizado del DEM SRTM 30 m
dem_orig   <- ee$Image("USGS/SRTMGL1_003")$rename("DEM")$clip(estudio_ee)
gauss      <- ee$Kernel$gaussian(radius=3, sigma=2, units="pixels", normalize=TRUE)
dem_smooth <- dem_orig$convolve(gauss)$resample("bilinear")
# 1. Centrar el visor en tu área de interés
Map$setCenter(lon = -74, lat = 4, zoom = 6)
# 2. Añadir el DEM original (sin suavizar)
Map$addLayer(
dem_orig,
visParams = list(min = 0, max = 3000,
palette = c("blue","green","yellow","red")),
name = "DEM original"
)
# 3. Añadir el DEM suavizado
Map$addLayer(
dem_smooth,
visParams = list(min = 0, max = 3000,
palette = c("blue","green","yellow","red")),
name = "DEM suavizado"
)
# 7. Importar y ejecutar TAGEE en GEE para atributos de terreno
tagee      <- ee$call("require", "users/zecojls/TAGEE:TAGEE-functions")
reticulate::py_last_error()
#Para exportar como .R plano
# knitr::purl('05_analisis_glm_hotspots.qmd')
if (!"pacman" %in% installed.packages()) install.packages("pacman")
pacman::p_load(here, remotes, sf, geojsonio, dplyr, ggplot2,
patchwork, wesanderson, qs)
# Selección entorno ya existente antes de cualquier llamado que use Python
reticulate::use_condaenv("rgee_py", required = TRUE)
## Librerías que usan Python
library(reticulate)
library(rgee)
library(googledrive)
# ==== Autenticación y backend Python ====
ee_clean_user_credentials()      # Limpia credenciales de GEE
ee_clean_pyenv()           # Limpia variables de entorno de reticulate
reticulate::py_run_string("import ee; ee.Authenticate()")
reticulate::py_run_string("import ee; ee.Initialize(project='even-electron-461718-g2')")
# Se consultan datos de DEM
img <- ee$Image("USGS/SRTMGL1_003")
#Consulta que propiedades están disponibles
img$propertyNames()$getInfo()
# Consultar una propiedad específica, e.g. keywords
img$get("keywords")$getInfo()
# Corre script externo para cargar
source(here::here("Scripts", "00_funcion_carga_ucs_procesadas_qs.R"), encoding = "UTF-8")
# Asignación de id único para cada polígono
ucs_rao_sf <- ucs_rao_sf |>
sf::st_make_valid() |> #valida geometrias problemáticas
dplyr::select(id_creado, UCSuelo, AREA_HA)
ucs_rao_sf <- ucs_rao_sf[!st_is_empty(ucs_rao_sf), ]
plot(st_geometry(ucs_rao_sf))
#Para exportar como .R plano
# knitr::purl('05_analisis_glm_hotspots.qmd')
if (!"pacman" %in% installed.packages()) install.packages("pacman")
pacman::p_load(here, remotes, sf, geojsonio, geojsonsf, dplyr, ggplot2,
patchwork, wesanderson, qs)
# Selección entorno ya existente antes de cualquier llamado que use Python
reticulate::use_condaenv("rgee_py", required = TRUE)
## Librerías que usan Python
library(reticulate)
library(rgee)
library(googledrive)
# ==== Autenticación y backend Python ====
ee_clean_user_credentials()      # Limpia credenciales de GEE
ee_clean_pyenv()           # Limpia variables de entorno de reticulate
reticulate::py_run_string("import ee; ee.Authenticate()")
reticulate::py_run_string("import ee; ee.Initialize(project='even-electron-461718-g2')")
# === Autenticación Google Drive ===
googledrive::drive_auth()
# Se consultan datos de DEM
img <- ee$Image("USGS/SRTMGL1_003")
#Consulta que propiedades están disponibles
img$propertyNames()$getInfo()
# Consultar una propiedad específica, e.g. keywords
img$get("keywords")$getInfo()
pacman::p_load(here, remotes, sf, geojsonio, geojsonsf, dplyr, purrr, ggplot2,
patchwork, wesanderson, qs)
combinar_y_subir_csv <- function(propiedad,
carpeta_drive_id_origen = "17yxwhlpgL4EG8inI5u8Nwi08wOrnhJiM",  # GEE_exports
carpeta_drive_id_destino = "1qJ5S25TZaFWzueNhx1M4Gr3P8JYWKGeU",  # Proyecto
carpeta_temporal = "tmp_csv") {
# Crea carpeta temporal local si no existe
if (!dir.exists(carpeta_temporal)) {
dir.create(carpeta_temporal)
}
# Listar archivos en Google Drive (solo .csv con prefijo exacto)
archivos_drive <- googledrive::drive_ls(
path = as_id(carpeta_drive_id_origen),
pattern = glue::glue("^{propiedad}_.*\\.csv$")
) |>
dplyr::filter(stringr::str_ends(name, ".csv"))
if (nrow(archivos_drive) == 0) {
stop(glue::glue("No se encontraron archivos CSV para la propiedad '{propiedad}' en GEE_exports."))
}
message(glue::glue("📥 Descargando {nrow(archivos_drive)} archivos CSV para '{propiedad}'..."))
# Descargar archivos al directorio temporal
purrr::walk2(
archivos_drive$name,
archivos_drive$id,
~ googledrive::drive_download(
file = as_id(.y),
path = file.path(carpeta_temporal, .x),
overwrite = TRUE,
quiet = TRUE
)
)
# Leer y combinar
archivos_locales <- list.files(path = carpeta_temporal,
pattern = paste0("^", propiedad, "_.*\\.csv$"),
full.names = TRUE)
combinado <- purrr::map_dfr(archivos_locales, readr::read_csv, show_col_types = FALSE)
# Escribir archivo combinado
nombre_salida <- paste0("OUT_", propiedad, "_combinado.csv")
ruta_salida <- file.path(carpeta_temporal, nombre_salida)
readr::write_csv(combinado, ruta_salida)
# Subir a carpeta final de proyecto en Drive
archivo_subido <- googledrive::drive_upload(
media = ruta_salida,
path = as_id(carpeta_drive_id_destino),
name = nombre_salida,
overwrite = TRUE
)
message(glue::glue("✅ Archivo combinado subido: {archivo_subido$name} (ID: {archivo_subido$id})"))
# Limpieza automática
unlink(carpeta_temporal, recursive = TRUE)
message("🧹 Archivos temporales eliminados.")
}
combinar_y_subir_csv("DEM")
rlang::last_trace()
combinar_y_subir_csv <- function(propiedad,
carpeta_drive_id_origen = "17yxwhlpgL4EG8inI5u8Nwi08wOrnhJiM",  # GEE_exports
carpeta_drive_id_destino = "1qJ5S25TZaFWzueNhx1M4Gr3P8JYWKGeU",  # Proyecto
carpeta_temporal = "tmp_csv") {
# Crea carpeta temporal local si no existe
if (!dir.exists(carpeta_temporal)) {
dir.create(carpeta_temporal)
}
# Listar archivos en Google Drive (solo .csv con prefijo exacto)
archivos_drive <- googledrive::drive_ls(
path = as_id(carpeta_drive_id_origen),
pattern = glue::glue("^{propiedad}_.*\\.csv$")
) |>
dplyr::filter(stringr::str_ends(name, ".csv"))
if (nrow(archivos_drive) == 0) {
stop(glue::glue("No se encontraron archivos CSV para la propiedad '{propiedad}' en GEE_exports."))
}
message(glue::glue("📥 Descargando {nrow(archivos_drive)} archivos CSV para '{propiedad}'..."))
# Descargar archivos al directorio temporal
purrr::walk2(
archivos_drive$name,
archivos_drive$id,
~ googledrive::drive_download(
file = as_id(.y),
path = file.path(carpeta_temporal, .x),
overwrite = TRUE
)
)
# Leer y combinar
archivos_locales <- list.files(path = carpeta_temporal,
pattern = paste0("^", propiedad, "_.*\\.csv$"),
full.names = TRUE)
combinado <- purrr::map_dfr(archivos_locales, readr::read_csv, show_col_types = FALSE)
# Escribir archivo combinado
nombre_salida <- paste0("OUT_", propiedad, "_combinado.csv")
ruta_salida <- file.path(carpeta_temporal, nombre_salida)
readr::write_csv(combinado, ruta_salida)
# Subir a carpeta final de proyecto en Drive
archivo_subido <- googledrive::drive_upload(
media = ruta_salida,
path = as_id(carpeta_drive_id_destino),
name = nombre_salida,
overwrite = TRUE
)
message(glue::glue("✅ Archivo combinado subido: {archivo_subido$name} (ID: {archivo_subido$id})"))
# Limpieza automática
unlink(carpeta_temporal, recursive = TRUE)
message("🧹 Archivos temporales eliminados.")
}
combinar_y_subir_csv("DEM")
combinar_y_subir_csv("SLOPE")
# Ruta al CSV combinado
dem_cv <- read_csv("tmp_csv/OUT_DEM_combinado.csv", show_col_types = FALSE)
# Convierte geometría desde .geo (GeoJSON como texto) a objeto sf
dem_cv_sf <- st_as_sf(
data.frame(dem_cv, geometry = geojson_sf(dem_cv$.geo)), #convierte a sf
crs = 4326) |>
select(-.geo) |> #elimina columna de geometria obsoleta
mutate(cv = stdDev / mean) #calcula coeficiente de variación
# Ruta al CSV combinado
dem_cv <- read_csv(here::here("Data", "OUT_DEM_combinado" ))
# Convierte geometría desde .geo (GeoJSON como texto) a objeto sf
dem_cv_sf <- st_as_sf(
data.frame(dem_cv, geometry = geojson_sf(dem_cv$.geo)), #convierte a sf
crs = 4326) |>
select(-.geo) |> #elimina columna de geometria obsoleta
mutate(cv = stdDev / mean) #calcula coeficiente de variación
# Ruta al CSV combinado
dem_cv <- read_csv(here::here("Data", "OUT_DEM_combinado.csv" ))
# Convierte geometría desde .geo (GeoJSON como texto) a objeto sf
dem_cv_sf <- st_as_sf(
data.frame(dem_cv, geometry = geojson_sf(dem_cv$.geo)), #convierte a sf
crs = 4326) |>
select(-.geo) |> #elimina columna de geometria obsoleta
mutate(cv = stdDev / mean) #calcula coeficiente de variación
pacman::p_load(here, remotes, sf, geojsonio, geojsonsf, dplyr, purrr, readr, ggplot2,
patchwork, wesanderson, qs)
# Ruta al CSV combinado
dem_cv <- read_csv(here::here("Data", "OUT_DEM_combinado.csv" ))
# Ruta al CSV combinado
dem_cv <- read_csv(here::here("Data", "OUT_covars_csv","OUT_DEM_combinado.csv" ))
# Convierte geometría desde .geo (GeoJSON como texto) a objeto sf
dem_cv_sf <- st_as_sf(
data.frame(dem_cv, geometry = geojson_sf(dem_cv$.geo)), #convierte a sf
crs = 4326) |>
select(-.geo) |> #elimina columna de geometria obsoleta
mutate(cv = stdDev / mean) #calcula coeficiente de variación
View(dem_cv_sf)
47.91928/1984.036
# Ruta al CSV combinado
slope_cv <- read_csv(here::here("Data", "OUT_covars_csv","OUT_SLOPE_combinado.csv" ))
# Convierte geometría desde .geo (GeoJSON como texto) a objeto sf
slope_cv_sf <- st_as_sf(
data.frame(slope_cv, geometry = geojson_sf(slope_cv$.geo)), #convierte a sf
crs = 4326) |>
select(-.geo) |> #elimina columna de geometria obsoleta
mutate(cv = stdDev / mean) #calcula coeficiente de variación
# Corre script externo para cargar
source(here::here("Scripts", "00_funcion_carga_ucs_procesadas_qs.R"), encoding = "UTF-8")
# Asignación de id único para cada polígono
ucs_rao_sf <- ucs_rao_sf |>
sf::st_make_valid() |> #valida geometrias problemáticas
dplyr::select(id_creado, UCSuelo, AREA_HA)
ucs_rao_sf <- ucs_rao_sf[!st_is_empty(ucs_rao_sf), ]
#Se verifica visulalmente
ggplot(data = ucs_rao_sf) +
geom_sf(aes(fill = UCSuelo), color = NA) +
theme_void() +
theme(legend.position = "none")
# Ruta al CSV combinado
dem_cv <- read_csv(here::here("Data", "OUT_covars_csv","OUT_DEM_combinado.csv" ))
# Convierte geometría desde .geo (GeoJSON como texto) a objeto sf
dem_cv_sf <- st_as_sf(
data.frame(dem_cv, geometry = geojson_sf(dem_cv$.geo)), #convierte a sf
crs = 4326) |>
select(-.geo) |> #elimina columna de geometria obsoleta
mutate(cv = stdDev / mean) #calcula coeficiente de variación
# Ruta al CSV combinado
slope_cv <- read_csv(here::here("Data", "OUT_covars_csv","OUT_SLOPE_combinado.csv" ))
# Convierte geometría desde .geo (GeoJSON como texto) a objeto sf
slope_cv_sf <- st_as_sf(
data.frame(slope_cv, geometry = geojson_sf(slope_cv$.geo)), #convierte a sf
crs = 4326) |>
select(-.geo) |> #elimina columna de geometria obsoleta
mutate(cv = stdDev / mean) #calcula coeficiente de variación
str(ucs_rao_sf)
View(ucs_rao_sf)
ucs_rao_sf <- ucs_rao_sf |>
sf::st_make_valid()
View(ucs_rao_sf)
# Corre script externo para cargar
source(here::here("Scripts", "00_funcion_carga_ucs_procesadas_qs.R"), encoding = "UTF-8")
View(ucs_rao_sf)
# Asignación de id único para cada polígono
ucs_rao_sf <- ucs_rao_sf |>
sf::st_make_valid() |> #valida geometrias problemáticas
dplyr::select(id_creado, UCSuelo, AREA_HA, Q)
# Inicializa vector de estado
status_vector <- rep(NA_character_, nrow(dem_cv_sf))
# Recorre el log y asigna el estado a cada fila según rango
for (i in seq_len(nrow(registro_dem))) {
fila_inicio <- registro_dem$start_idx[i]
fila_fin    <- registro_dem$end_idx[i]
status_val  <- registro_dem$status[i]
status_vector[fila_inicio:fila_fin] <- status_val
}
# Añadir al objeto sf
dem_cv_sf$status <- status_vector
# Ruta al CSV combinado
dem_cv <- read_csv(here::here("Data", "OUT_covars_csv","OUT_DEM_combinado.csv" ))
# Convierte geometría desde .geo (GeoJSON como texto) a objeto sf
dem_cv_sf <- st_as_sf(
data.frame(dem_cv, geometry = geojson_sf(dem_cv$.geo)), #convierte a sf
crs = 4326) |>
select(-.geo) |> #elimina columna de geometria obsoleta
mutate(cv = stdDev / mean) #calcula coeficiente de variación
# Ruta al CSV combinado
slope_cv <- read_csv(here::here("Data", "OUT_covars_csv","OUT_SLOPE_combinado.csv" ))
# Convierte geometría desde .geo (GeoJSON como texto) a objeto sf
slope_cv_sf <- st_as_sf(
data.frame(slope_cv, geometry = geojson_sf(slope_cv$.geo)), #convierte a sf
crs = 4326) |>
select(-.geo) |> #elimina columna de geometria obsoleta
mutate(cv = stdDev / mean) #calcula coeficiente de variación
str(ucs_rao_sf)
str(dem_cv_sf)
str(slope_cv_sf)
View(slope_cv_sf)
