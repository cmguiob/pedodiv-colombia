---
title: "Procesamiento indice Rao a nivel nacional"
output: github_document
---

Este cuaderno de código raliza el procesamiento de datos del mapa de unidades cartográficas de suelos de Colombia a escala 1:100.000 del IGAC, el cual consta de 72903 entradas (multipoligonos). El producto final del script es un conjunto de datos con el índice de pedodiversidad de rao por polígono para toda Colombia. La carga se realiza ejecutando un script externo via ´source´.

## 1. Configuración

A continuación se cargan las librería necesarias.

```{r config, include = FALSE}

# Para renderizar el documento
knitr::opts_chunk$set(echo = TRUE)

# Para cargar librerias se verifica pacman
if ("pacman" %in% installed.packages() == FALSE) install.packages("pacman")

# Se cargan las librerias
pacman::p_load(char = c(
  "here", #manejo de rutas
  "sf", #manipulación de dats espaciales
  "tidyr", #limpieza de data frames
  "dplyr", #procesamiento de data frames
  "stringr", #procesamiento de texto
  "purrr",  #funciones de iteración en data frames
  "SoilTaxonomy", #análisis de taxonomia de suelos
  "ggplot2",  #graficación
  "patchwork", #mosaicos gráficos
  "wesanderson", #paleta de colores
  "qs" #escribir y leer rápidamente objetos R
  )
)

```

## 2. Carga de datos

Primero se corre un script externo para carga de datos masiva de UCS de Colombia desde Zenodo, Puede tardar algunos minutos.
```{r load_smu}
# Corre script externo 
source(here::here("Scripts", "carga_ucs_armonizadas_gpkg.R"), encoding = "UTF-8")

```

Se verifica visualmente la carga. En est caso, para ilustrar las áreas.

```{r check_visually}

# Se visualizan datos geográficamente (edad, sin leyenda)
ggplot2::ggplot() +
  geom_sf(data = ucs_sf, aes(fill = AREA_HA), color = NA) +
  scale_fill_viridis_c(name = "Área (Ha)", trans = "log10")+ 
  theme_minimal()

#guarda el último gráfico generado
ggsave(here("Figures","ucs_area_log10.png"), width = 10, height = 8, dpi = 300)

```


Luego se cargan los departamentos de Colombia

```{r load_departments}
# Define ruta y nombre de capa de geopackage de departamentos
deptos_ruta <- here("Data", "INPUT_departamentos_IGAC_Abril_2025.gpkg")
capa_nombre_deptos <- sf::st_layers(deptos_ruta)$name[1]

# Carga geopackage de dpartamentos
departamentos_sf <- sf::st_read(
  deptos_ruta,
  layer = capa_nombre_deptos,
  quiet = TRUE
)

# Verifica CRS de departamentps
crs_deptos <- sf::st_crs(departamentos_sf)
message("CRS detectado: ", crs_deptos$input, " (EPSG:", crs_deptos$epsg, ")")
```
Se verifica visualmente la carga de departamentos

```{r}

ggplot2::ggplot() +
  geom_sf(data = departamentos_sf, aes(fill = DeCodigo), color = NA) +
  guides(fill = guide_legend(title="Código departamento")) +
  theme_minimal()
```

## 3. Pre-procesamiento espacial

A continuación enriquecemos los datos de ucs adicionando la columna de departamento. Esto nos permite adicionalmente hacer filtrado espacial para trabajar en modo piloto por departamento, abordando así errores de forma gradual. 

Primero, se estandarizan nombres de columnas y se crean identificadores únicos para los datos.

```{r}
# ucs_sf pasa de "geom" a "geometry"
names(ucs_sf)[names(ucs_sf) == "geom"] <- "geometry"
ucs_sf <- sf::st_as_sf(as.data.frame(ucs_sf), sf_column_name = "geometry")

#se crean ids para las ucs
ucs_sf <- ucs_sf |>
  mutate(id_creado = row_number())

# departamento_1_sf pasa de "SHAPE" a "geometry"
names(departamentos_sf)[names(departamentos_sf) == "SHAPE"] <- "geometry"
departamentos_sf <- sf::st_as_sf(as.data.frame(departamentos_sf), sf_column_name = "geometry")

```

Hacemos un join espacial para adicionar la columna de departamentos a los datos de ucs. Dado que algunas ucs pueden estar en varios departamentos, se aplica un criterio simple para etiquetar la ucs con el departamento con el que se intersecta primero en los datos.

```{r}

# Aseguramos que ambos datasets tengan la misma proyección
ucs_sf <- st_transform(ucs_sf, st_crs(departamentos_sf))

# Hacemos el join espacial
ucs_deptos_sf <- st_join(ucs_sf, departamentos_sf |> select(DeNombre))

# Eliminamos posibles duplicados: quedamos con una fila por unidad
ucs_deptos_sf <- ucs_deptos_sf %>%
  group_by(id_creado) %>%
  slice(1) %>%
  ungroup()

```

Verificamos visualmente que todas las ucs tengan un departamento asignado

```{r}
ggplot2::ggplot() +
  geom_sf(data = ucs_deptos_sf, aes(fill = factor(DeNombre)), color = NA) +
  theme_minimal() +
  theme(legend.position = "none")
```


Se observa que algunos departamentos quedan discontínuos Seleccionamos un departamento para trabajar de forma piloto

```{r}
rm(list = setdiff(ls(), c("ucs_deptos_sf", "ucs_sf", "departamentos_sf")))

# Selección de departamento
ucs_depto_piloto_sf <- ucs_deptos_sf |>
  dplyr::filter(DeNombre %in% c(
    "Antioquia", 
    "Atlántico",
    "Bolívar",
    "Boyacá",
    "Caldas",
    "Cauca",
    "Cesar",
    "Chocó", 
    "Córdoba",
    "Cundinamarca",
    "Huila",
    "La Guajira",
    "Magdalena",
    "Nariño",
    "Norte de Santander",
    "Quindío",
    "Risaralda",
    "Santander", 
    "Sucre",
    "Tolima",
    "Valle del Cauca")
    ) |>
  tidyr::drop_na()

# Número de ucs únicas en el departamento
n_ucs_depto_piloto <- ucs_depto_piloto_sf |> select(UCSuelo) |> st_drop_geometry() |> distinct() |> count()

txto_n_ucs_depto_piloto <- paste("ucs = ", n_ucs_depto_piloto)

```

Se visualiza la seleción

```{r}

p_ucs_piloto <- ggplot() +
  geom_sf(data = departamentos_sf, fill = "gray90", color = "gray90") +
  geom_sf(data = ucs_depto_piloto_sf, aes(fill = factor(UCSuelo)), color = NA) +
  annotate("text", 
           x = bbox["xmax"] - 0.7 * (bbox["xmax"] - bbox["xmin"]),  # 70% desde el borde izquierdo
           y = bbox["ymax"] - 0.01 * (bbox["ymax"] - bbox["ymin"]),  # 1% desde el borde superior
           label = txto_n_ucs_depto_piloto,
           hjust = 1, vjust = 1, size = 5, color = "black") +
  theme_minimal(base_size = 14) +
  theme(
    legend.position = "none",
    axis.title = element_blank()
  )


# Guarda la figura como PNG
ggsave(
  filename = here("Figures", "ucs_piloto.png"),
  plot = p_ucs_piloto,
  width = 8,
  height = 10,
  dpi = 350
)
```


## 4. Procesamiento

A continuación se realiza el procesamiento para obtener las métricas de pedodiversidad

### 4.1 Extracción de prefijos taxonómicos

Seleccionamos datos que tengan formato válido.

```{r filter_valid_components}

palabras_problema <- c(
  "Cuerpo",
  "Cuerpos",
  "Misceláneos", 
  "Misceláneo", 
  "Afloramientos", 
  "Afloramiento", 
  "Vegetacion",
  "Vegetación", 
  "Otros", 
  "Zona",
  "Zonas",
  "Roca",
  "Afloramientos Rocosos",
  "Misceláneo de playas",
  "Pantanos y marismas",
  "Misceláneo arenoso",
  "Nieves Perpetuas",
  "No aplica",
  "Pantanos",
  "Tierras de carcavas",
  "Tierra de carcavas",
  "Nieves",
  "arenas volcánicas y depósitos piroclásticos",
  "Misceláneo de cenizas",
  "Fosa Mina Carbón",
  "Saladares",
  "Tierra Relave Carbón"
)

datos_subgrupo_validos <- ucs_depto_piloto_sf |>
  mutate(
    # regex para eliminar términos problemáticos en cualquier posición dentro de SUBGRUPO:
    # (?i) -> modo case-insensitive (mayúsc/minúsc no importa)
    # (^|,\s*) -> inicio del texto (^) o una coma seguida opcionalmente de espacios (", ")
    # (palabras_problema)-> cualquiera de las palabras/frases listadas, separadas por "|"
    # (?=,|$) -> lookahead para asegurar que justo después venga una coma o el final de texto 
    SUBGRUPO = str_remove_all(
      SUBGRUPO,
      str_c("(?i)(^|,\\s*)(", str_c(palabras_problema, collapse = "|"), ")(?=,|$)")
    ),
    
    # Limpia espacios duplicados
    SUBGRUPO = str_squish(SUBGRUPO),
    # Reemplaza comas consecutivas con una sola coma
    SUBGRUPO = str_replace_all(SUBGRUPO, ",{2,}", ","),
    # Elimina comas iniciales o finales sobrantes
    SUBGRUPO = str_replace_all(SUBGRUPO, "^,\\s*|,\\s*$", "")
  ) |>
  mutate(
    # Extrae la primera palabra para facilitar filtros
    primer_palabra = word(SUBGRUPO, 1)
  ) |>
  filter(
    # Filtra filas con SUBGRUPO no vacío y que no comiencen con palabra problemática,
    # o que contengan múltiples componentes (separados por ";")
    str_trim(SUBGRUPO) != "",
    !primer_palabra %in% palabras_problema | str_detect(SUBGRUPO, ";")
  ) |>
  # Elimina la columna auxiliar 'primer_palabra' que ya no se necesita
  select(-primer_palabra) |>
  # Quita la geometría del objeto sf para evitar problemas en pasos posteriores
  sf::st_drop_geometry()

```

Se inspeccionan los datos excluidos. 

*Esto se tiene que reevaluar, para que no queden huecos, no deberian quitarse, sino procesarse distinto.*

```{r check_excluded_components}

# Segundo, filtrar los datos que fueron excluidos
datos_excluidos <- ucs_depto_piloto_sf |>
  st_drop_geometry() |>
  anti_join(datos_subgrupo_validos, by = "id_creado")

```


Se verifica que las proporciones no superen el número de taxones reportados en cada UCS. 

```{r check_percentage}

datos_con_error <- datos_subgrupo_validos %>%
  mutate(
    #Divide el vector en una lista de partes separadas por ;, ignorando espacios después del ;
    componentes = str_split(SUBGRUPO, ",\\s*"),
    porcentajes = str_split(PORCENTAJE, ",\\s*"),
    #Cuenta cuántos componentes tiene cada elemento de la lista
    n_componentes = map_int(componentes, length),
    n_porcentajes = map_int(porcentajes, length)
  ) %>%
  # Filtra las filas donde el número de componentes difiere del de porcentajes
  filter(n_componentes != n_porcentajes)

# Mostrar las UCS problemáticas
print(datos_con_error %>% select(id_creado, SUBGRUPO, PORCENTAJE))

```

En el bloque anterior se identificaron casos en que el numero de porcentajes y de taxones reportados difiern. También casos en que no se asignan porcentajes (dice "No aplica"), a pesar de haber reportado taxones. En este último caso, se podrían asignar porcentajes en igual proporción a los taxones reportados.

En algunos casos el numero de proporciones son mayores, en otras menores. Son mayores, por ejemplo, en casos en que se removió "Afloramientos Rocosos" y este contaba como proporción. Aunque también ocurre el caso inverso, en que aparecía pero no lo contaban como proporción. En cualquier caso, al no constituir un tipo de suelo, no refleja pedodiversidad.

*ojo, toca ajustar función para que etiquete no solo a los que se les asignó igual proporción, sino también a los que se escalaron*

Se crea una función para corregir porcentajes de acuerdo a estos criterios:

```{r}

corregir_porcentajes <- function(p_string, n_taxones) {
  # Si el string está vacío, es NA o contiene "No aplica" (sin importar mayúsculas)
  # Se asignan porcentajes iguales entre todos los taxones
  if (is.na(p_string) || p_string == "" || str_detect(p_string, regex("no aplica", ignore_case = TRUE))) {
    p_num <- rep(100 / n_taxones, n_taxones)
    es_asignacion_igual <- TRUE

  } else {
    # Separar el string por comas y limpiar espacios
    p <- str_trim(str_split(p_string, ",\\s*")[[1]])

    # Convertir a numérico suprimiendo advertencias (por textos no numéricos)
    p_num <- suppressWarnings(as.numeric(p))

    # Si todos los valores son NA después de convertir, se asignan valores iguales
    if (all(is.na(p_num))) {
      p_num <- rep(100 / n_taxones, n_taxones)
      es_asignacion_igual <- TRUE

    } else {
      es_asignacion_igual <- FALSE

      # --- CASOS DE CORRECCIÓN DE LONGITUDES ---

      # Caso 1: hay un porcentaje de más → eliminar el último
      if (length(p_num) == n_taxones + 1) {
        p_num <- p_num[1:n_taxones]

      # Caso 2: hay un porcentaje de menos → calcular faltante para completar 100
      } else if (length(p_num) == n_taxones - 1) {
        faltante <- 100 - sum(p_num, na.rm = TRUE)
        p_num <- c(p_num, faltante)

      # Caso 3: hay más de un porcentaje faltante → repartir restante equitativamente
      } else if (length(p_num) < n_taxones) {
        suma_actual <- sum(p_num, na.rm = TRUE)
        n_faltantes <- n_taxones - length(p_num)
        restante <- 100 - suma_actual

        if (restante >= 0) {
          # Reparto equitativo del porcentaje restante
          p_num <- c(p_num, rep(restante / n_faltantes, n_faltantes))
        } else {
          warning("Suma de porcentajes excede 100, no se puede completar los valores faltantes")
          return(list(porcentajes = rep(NA_real_, n_taxones), es_asignacion_igual = es_asignacion_igual))
        }
      }

      # --- VALIDACIÓN FINAL ---

      # Si aún no coincide la longitud, abortar y devolver NAs
      if (length(p_num) != n_taxones) {
        warning("Número de porcentajes no coincide con número de taxones")
        return(list(porcentajes = rep(NA_real_, n_taxones), es_asignacion_igual = es_asignacion_igual))
      }

      # --- ESCALADO FINAL ---

      # Ajustar todos los valores proporcionalmente para que sumen exactamente 100
      suma_total <- sum(p_num, na.rm = TRUE)
      if (!is.na(suma_total) && suma_total != 100 && suma_total > 0) {
        p_num <- 100 * p_num / suma_total
      }
      # Redondear a 4 decimales para evitar artefactos de coma flotante
      p_num <- round(p_num, 4)
    }
  }

  # --- SALIDA ---

  return(list(
    porcentajes = p_num,
    es_asignacion_igual = es_asignacion_igual
  ))
}


```


A continuación se procesan los datos para separar componentes

```{r}

datos_largos <- datos_subgrupo_validos %>%
  # Separa los componentes en listas por ",", ";" o "."
  mutate(componentes = str_split(SUBGRUPO, ",\\s*|;\\s*|\\.\\s*"),
         # Cuenta el número de componentes por fila
         n_componentes = map_int(componentes, length),
         # Aplica la función corregir_porcentajes que devuelve lista con porcentajes y flag
         correcciones = map2(PORCENTAJE, n_componentes, corregir_porcentajes),
         # Extrae los porcentajes corregidos
         porcentajes = map(correcciones, "porcentajes"),
         # Extrae el flag que indica asignación automática
         asignacion_igual = map_lgl(correcciones, "es_asignacion_igual"),
         # Cuenta los porcentajes corregidos
         n_porcentajes = map_int(porcentajes, length)) %>%
  # Filtra filas donde número de componentes y porcentajes coinciden
  filter(n_componentes == n_porcentajes) %>%
  # Expande listas de componentes y porcentajes a filas largas
  unnest(c(componentes, porcentajes)) %>%
  # Limpia espacios y convierte porcentajes a numérico
  mutate(nombre_taxonomico = str_trim(componentes),
         porcentaje = porcentajes) %>%
  # Selecciona columnas relevantes, incluyendo el flag para seguimiento
  select(id_creado, nombre_taxonomico, porcentaje, asignacion_igual)

```

Se añaden niveles taxonomicos usando SoilTaxonomy

```{r}
# Añadir niveles taxonómicos usando SoilTaxonomy
datos_niveles <- datos_largos %>%
  mutate(
    orden = map_chr(nombre_taxonomico, ~getTaxonAtLevel(.x, level = "order")),
    suborden = map_chr(nombre_taxonomico, ~getTaxonAtLevel(.x, level = "suborder")),
    gran_grupo = map_chr(nombre_taxonomico, ~getTaxonAtLevel(.x, level = "greatgroup")),
    subgrupo = map_chr(nombre_taxonomico, ~getTaxonAtLevel(.x, level = "subgroup"))
  )
```

Se hace un chequeo para revisar si algunos nombres no pudieron procesarse. Se filtran los datos con NAs. Esto debido a que posteriormente se crea una función para extraer prefijos, la cual retorna error si hay NAs.

```{r}

datos_NA_niveles <- datos_niveles |>
  filter(is.na(orden) | is.na(suborden) | is.na(gran_grupo) | is.na(subgrupo))

```

Se corrigen errores de ortografía. Para identificarlos, existe un script externo, que usa la librería ´SoilTaxonomy´ donde se pueden verificar las opciones de subgrupo disponible para el nivel de grupo.

```{r correct_spelling}

correcciones <- c(
  #Vichada
  "Oxiaquic" = "Oxyaquic",
  "Xantic" = "Xanthic",
  #Antioquia
  "Fluvaquentic Dystrustepts" = "Fluventic Dystrustepts", # revisar nombre completo
  "Fluvaquentic Dystrudepts" = "Fluventic Dystrudepts",
  "Fluvaquentic Haplustepts" = "Fluventic Haplustepts",
  #"Fluventicc" = "Fluventic",
  "Usthorthents" =  "Ustorthents",
  "Hapludoxs" = "Hapludox",
  "Kandiudoxs" = "Kandiudox",
  "Haplofibrist" = "Haplofibrists",
  "Haplofibristss" = "Haplofibrists",
  #Boyacá
  "Ruptic Ultic Dystrudepts" = "ruptic-ultic dystrudepts",
  #Chocó
  "Plintic" = "Plinthic",
  "Virtrandic" = "Vitrandic",
  #Cesar
  "Aridic Entic Haplustolls" = "aridic lithic haplustolls",
  #Córdoba
  "Aquertic Dystrustepts" = "Aquic Dystrustepts",
  "Aquertic Haplustepts" = "Aquic Haplustepts",
  "Rudic Ultic Eutrudepts" = "Ruptic-Alfic Eutrudepts",
  #Cundinamarca
  "Humic Dystrocryepts" = "Folistic Dystrocryepts",
  "Humic Lithic Dystrocryepts" = "Lithic Dystrocryepts",
  "Humic Lithic Dystrustepts" = "Lithic Dystrustepts",
  #Huila
  "Aquic Endoaquepts" = "Typic Endoaquepts",
  "Humic Haplustepts" = "Typic Haplustepts",        # o "Humic Dystrudepts" si se justifica
  "Plinthic Endoaquepts"     = "Typic Endoaquepts", #tampoco Plnthaquic Endoaquepts en la librería
  #La Guajira
  "Lithic Aridic Ustorthents" = "aridic lithic ustorthents",
  "Saladic Natrustalfs" = "salidic natrustalfs",
  "Inceptic Aridic Haplustalfs" = "inceptic haplustalfs",
  #Magdalena
  "Thapto Histic Fluvaquents" = "thapto-histic fluvaquents",
  #Norte de Santander
  "Entic Dystrudepts" = "Typic Dystrudepts",
  #Santander
  "Ustic Dystrustepts" = "Typic Dystrustepts",
  "Ustic Dystrudepts" = "Typic Dystrustepts",
  "Humic Pachic Dystrudepts" = "Humic Dystrudepts",
  #Sucre
  "Aquic Haplusterts" = "Udic Haplusterts",
  "Vertic Epiaquerts" = "Typic Epiaquerts",
  #Tlima
  "Sulfic Ustifluvents" = "Typic Ustifluvents",
  #Valle del Cauca
  "Hapludoxs" = "Hapludox",
  "Fluvaquentic Endoaquerts" = "Typic Endoaquerts",
  "Thapto Histic Sulfaquents" = "thapto-histic sulfaquents")

# Correcciones manuales conocidas (puedes agregar más si aparecen otros errores)
datos_largos <- datos_largos |>
  mutate(nombre_taxonomico = str_replace_all(nombre_taxonomico, correcciones))
```

Se añaden niveles taxonomicos usando SoilTaxonomy

```{r}
# Añadir niveles taxonómicos usando SoilTaxonomy
datos_niveles_limpios <- datos_largos %>%
  mutate(
    orden = map_chr(nombre_taxonomico, ~getTaxonAtLevel(.x, level = "order")),
    suborden = map_chr(nombre_taxonomico, ~getTaxonAtLevel(.x, level = "suborder")),
    gran_grupo = map_chr(nombre_taxonomico, ~getTaxonAtLevel(.x, level = "greatgroup")),
    subgrupo = map_chr(nombre_taxonomico, ~getTaxonAtLevel(.x, level = "subgroup"))
  )
```

A continuación se crea una función para poder extraer prefijos de los niveles jerárquicos de los nombres. Se utiliza la librería SoilTaxonomy.

```{r}

# Tabla de prefijos para órdenes principales (puedes ampliarla)
prefijos_orden <- tibble::tibble(
  orden = c("entisols", "inceptisols", "alfisols", "ultisols", "mollisols", "vertisols", "aridisols", "oxisols", "spodosols", "histosols", "andisols", "gelisols"),
  prefijo_orden = c("ents", "epts", "alfs", "ults", "olls", "erts", "ids", "ox", "ods", "ists", "ands", "els")
)

# Asocia prefijo de orden a los datos
datos_niveles_limpios <- datos_niveles_limpios %>%
  left_join(prefijos_orden, by = "orden")

# Define función para generar el prefijo quitando el string de la jerarquia anterior
extraer_prefijo_nuevo <- function(nombre_actual, anterior) {
  if (is.na(nombre_actual) | is.na(anterior)) {
    return(NA_character_)
  } else {
    nombre_minuscula <- tolower(nombre_actual)
    anterior_minuscula <- tolower(anterior)
    nuevo_prefijo <- str_remove(nombre_minuscula, fixed(anterior_minuscula))
    nuevo_prefijo <- str_extract(nuevo_prefijo, "^[a-z]+")
    return(nuevo_prefijo)
  }
}

```

Se extraen prefijos por niveles jerárquicos y se convierten porcentajes a proporciones. 

```{r}

datos_prefijos <- datos_niveles_limpios |>
  mutate(
    # El prefijo para suborden se obtiene quitando el prefijo del orden
    prefijo_suborden = map2_chr(suborden, prefijo_orden, extraer_prefijo_nuevo),
    # El prefijo para gran grupo se obtiene quitando el suborden
    prefijo_gran_grupo = map2_chr(gran_grupo, suborden, extraer_prefijo_nuevo),
    # El prefijo para subgrupo se obtiene quitando el gran grupo
    prefijo_subgrupo = map2_chr(subgrupo, gran_grupo, extraer_prefijo_nuevo),
    # convierte a proporción
    proporcion = porcentaje / 100
  ) |>
  group_by(id_creado) |>
  mutate(ID_INTERNO = row_number()) |>  # identificador único por taxón en cada unidad
  ungroup() |>
  select(-porcentaje)

```

### 4.2 Cálculo de distancias taxonómicas

Una vez se tienen los datos procesados, con prefijos taxonómicos separados y proporción por cada tipo de suelo, se procede a calcular distancias taxinómicas e índices.

```{r}

# Crear combinaciones de pares (i, j) correctamente
pares <- datos_prefijos |>
  group_by(id_creado) |>
  summarise(
    pares = list(tidyr::crossing(ID_1 = ID_INTERNO, ID_2 = ID_INTERNO)),
    .groups = "drop" #elimina los grupos que se hayan formado 
  ) |>
  unnest(pares)

# Ahora unir los datos de cada par i, j
pares_datos <- pares %>%
  left_join((datos_prefijos), 
            by = c("id_creado", "ID_1" = "ID_INTERNO")
            ) %>%
  left_join((datos_prefijos), 
            by = c("id_creado", "ID_2" = "ID_INTERNO"), 
            suffix = c("_i", "_j"))

```



Se crea función para comparar prefijos nivel a nivel y calcular d_ij

```{r}

comparar_prefijos <- function(row) {
  #Junta los prefijos del suelo i en un vector
  niveles_i <- c(row$prefijo_orden_i, row$prefijo_suborden_i, row$prefijo_gran_grupo_i, row$prefijo_subgrupo_i)
  #Junta los prefijos del suelo j en un vector
  niveles_j <- c(row$prefijo_orden_j, row$prefijo_suborden_j, row$prefijo_gran_grupo_j, row$prefijo_subgrupo_j)

  # Validar en qué niveles ambos tienen datos (no NA)
  niveles_validos <- !is.na(niveles_i) & !is.na(niveles_j)
  c <- sum(niveles_validos)  # número de niveles comparables

  coincidencias <- niveles_i == niveles_j  # TRUE si coinciden por nivel

  # No contar como coincidencia en subgrupo si es "typic" y los niveles anteriores son distintos
  if (!is.na(row$prefijo_subgrupo_i) && row$prefijo_subgrupo_i == "typic") {
    if (!is.na(row$prefijo_gran_grupo_i) && !is.na(row$prefijo_gran_grupo_j) &&
        row$prefijo_gran_grupo_i != row$prefijo_gran_grupo_j) {
      coincidencias[4] <- FALSE
    }
  }

  x <- sum(coincidencias & niveles_validos)  # número de niveles con coincidencia real

  dij <- if (c == 0) NA_real_ else (c - x) / c  # distancia d_ij

  return(list(dij = dij, c = c, x = x))  # devolver todo como lista
}

```

Se aplica función para  comparar fila por fila para calcular dij y pipj*dij

```{r}

resultado_df <- pares_datos |>
  rowwise() |>
  mutate(
    resultado = list(comparar_prefijos(cur_data())),
    dij       = resultado$dij,
    c         = resultado$c,
    x         = resultado$x,
    prod_pipj_dij = proporcion_i * proporcion_j * dij
  ) |>
  ungroup() |>
  select(-resultado,
         -ID_1,
         -ID_2,
         -orden_i,
         -orden_j, 
         -suborden_i, 
         -suborden_j,
         - gran_grupo_i, 
         - gran_grupo_j,
        - subgrupo_i, 
        - subgrupo_j
        )  

```

### 4.3 Cálculo de índices de diversidad

A continuación se calcula el índce de diversidad de Rao (Q)

```{r}

Q_por_ID <- resultado_df %>%
  group_by(id_creado) %>%
  summarise(
    Q = sum(prod_pipj_dij, na.rm = TRUE),
    asignacion_igual_i = first(asignacion_igual_i),  # asume que 'unidad' no varía dentro del grupo
    .groups = "drop"
  )

# Une los valores de Q con el objeto espacial
datos_sf_q <- ucs_depto_piloto_sf %>%
  left_join(Q_por_ID, by = "id_creado")

# Exporta los datos en formato qs
qs::qsave(datos_sf_q, here::here("Data", "OUTPUT_rao_piloto_sf.qs"))

#Alternativamente, si se desea exportar para SIG
#sf::st_write(
#  datos_sf_q, 
#  here::here("Data", "OUTPUT_rao_piloto_sf.gpkg"),
#  layer = "datos_sf_q", 
#  driver = "GPKG"
#)


```

Se visualizan los resultados espacialmente

```{r}

#Paleta de colores
pal <- wes_palette("Zissou1", 100, type = "continuous")

p_rao_piloto <- ggplot(datos_sf_q) +
  geom_sf(data = departamentos_sf, fill = "gray90", color = "gray90") +
  geom_sf(aes(fill = Q), color = NA) +
  scale_fill_gradientn(
    colours = pal,
    na.value = "white",
    guide = guide_colorbar(
      direction = "horizontal",
      title.position = "left",
      title.vjust = 1,             # vertical adjustment (optional)
      barwidth = unit(8, "cm"),
      barheight = unit(0.4, "cm")
    )
  ) +
  labs(fill = "Entropía Q") +
  theme_minimal(base_size = 14) +
  theme(
    legend.position = "bottom",
    legend.box = "horizontal",
    legend.justification = "center",
    legend.title.align = 0.5,
    legend.text.align = 0.5,
    legend.margin = margin(t = 15, unit = "pt")  # adds space above legend
  )

# Guarda la figura como PNG
ggsave(
  filename = here("Figures", "rao_piloto.png"),
  plot = p_rao_piloto,
  width = 8,
  height = 10,
  dpi = 300
)

#Visualiza
#p_rao_piloto

```

Se visualizan las áreas de las ucs para el área piloto

```{r check_visually}

# Se visualizan datos geográficamente (edad, sin leyenda)
p_ucs_area_piloto <- ggplot2::ggplot(datos_sf_q) +
  geom_sf(data = departamentos_sf, fill = "gray90", color = "gray90") +
  geom_sf(aes(fill = AREA_HA), color = NA) +
  scale_fill_viridis_c(
    name = "Área (Ha)",
    guide = guide_colorbar(
      direction = "horizontal",
      title.position = "left",     # Title on the left
      title.vjust = 1,             # vertical adjustment (optional)
      barwidth = unit(8, "cm"),    # longer bar
      barheight = unit(0.4, "cm")  # thinner bar
    )
  ) +
  theme_minimal(base_size = 14) +
  theme(
    legend.position = "bottom",
    legend.box = "horizontal",
    legend.justification = "center",
    legend.title.align = 0.5,
    legend.text.align = 0.5,
    legend.margin = margin(t = 15, unit = "pt")  # ← adds space above legend
  )

#guarda el último gráfico generado
ggsave(here("Figures","ucs_area_piloto.png"), 
       plot = p_ucs_area_piloto,
       width = 8, 
       height = 10, 
       dpi = 350)

```

Se crea el mosaico gráfico para exportar

```{r}

# Armar el mosaico horizontal de los tres mapas
p_mosaico_ucs_rao <- p_ucs_piloto + p_ucs_area_piloto + p_rao_piloto +
  plot_layout(ncol = 3)

# Exportar
ggsave(
  filename = here::here("Figures", "mosaico_ucs_rao.png"),
  plot = p_mosaico_ucs_rao,
  width = 21,
  height = 8,
  dpi = 350
)


#Se visualiza
#p_mosaico_ucs_rao

```













